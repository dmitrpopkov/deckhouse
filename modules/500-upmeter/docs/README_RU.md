---
title: "Модуль upmeter"
webIfaces:
- name: status
- name: upmeter
---

Модуль собирает статистику по типам доступности для компонентов кластера и Deckhouse. Позволяет оценивать степень выполнения SLA на эти компоненты, показывает данные о доступности в веб-интерфейсе и предоставляет веб-страницу статуса работы компонентов кластера.

С помощью custom resource [UpmeterRemoteWrite](cr.html#upmeterremotewrite) можно экспортировать метрики доступности по протоколу [Prometheus Remote Write](https://docs.sysdig.com/en/docs/installation/prometheus-remote-write/).

## Состав модуля

**Агент**, DaemonSet `upmeter-agent`

- запускает проверки и отправляет результаты измерений на сервер каждые 30 секунд
- работает только на мастер-узлах

**Сервер**, StatefulSet `upmeter`п

- принимает данные агентов
- агрегирует 30-секундные интервалы в 5-минутные
- экспортирует статистику в виде метрики по протоколу Prometheus Remote Write
- выступает API-сервером для веб-интерфейсов


**Статус-страница**, Deployment `status`

Показывает текущий уровень доступности за последние 10 минут. По умолчанию требует авторизации, но её можно отключить.

**Дашборд**, Deployment `webui`

Дашборд со статистикой по пробам и группам доступности. Можно выбирать разные периоды времени. Требует авторизации.

**Модельное приложение**, StatefulSet `smoke-mini-(a,b,c,d,e)`

Минимальное приложение, которое используется для постоянного smoke-тестирования сетевой связности
узлов.

### Интерфейс

Пример веб-интерфейса:
![Пример веб-интерфейса](../../images/500-upmeter/image1.png)

Пример графиков по метрикам из upmeter в Grafana:
![Пример графиков по метрикам из upmeter в Grafana](../../images/500-upmeter/image2.png)


## Концепция

### Термины

**Проверка** (check) — это способ измерения доступности конкретной функции приложения или компонента кластера.

**Проба** (probe) — это способ оценки доступности компонента кластера, компонента Декхауса или его функции. Проба состоит из одной или нескольких проверок.

**Группа доступности** (availablility group, group) — это группа проб, объединенная по принципу функциональности. Группа состоит из одной или более проб.  По группе оценивается доступность и соблюдение SLA согласно условиям соглашения.

Когда проверка завершается, она получает статус. Статус пробы вычисляется из статусов проверки, а статус группы — из статусов проб. Какие есть статусы:

* **Up** — проверка доступности положительная,
* **Down** — проверка доступности отрицательная,
* **Unknown** — данные собирались, но не смогли вынести вердикт доступности,
* **Nodata** — данные не собирались, например если upmeter-agent не работал.

Пример для статуса *Unknown*. Перед проверкой статуса Pod'а (Ready или нет), делается проверка доступности API-сервера — запрос на `/version`, как в пробе `control-plane/apiserver`. Если версию API-сервера получить не удалось, то проверка Pod'а возвращает статус *Unknown*, потому что статус Pod'а узнать невозможно.


### Вычисление статуса

Если проверки одной пробы завершились с разными статусами, то статус этой пробы вычисляется по присутствию наименьшего значения при отношении

    Down < Up < Unknown

Как это работает:

* Если хотя бы одна проверка в пробе имеет статус *Down*, то статус пробы *Down*
* Если нет статуса *Down* и есть хотя бы один статус *Up*, то статус пробы *Up*
* Если все проверки имеют статус *Unknown*, то статус пробы *Unknown*

Таким же способом складывается статус группы на основе статусов проб.

Агенты Апметра отправляют статистику доступности за 30 секунд на сервер. 30 секунд — это оперативный интервал. Интервалы накоплений определяются по системным часам с границами, кратными 30 секундам текущего времени. Статистика для групп вычисляется в агенте и отправляется на сервер совместно со статистикой для проб.

Сервер комбинирует статистику от одного или нескольких агентов в интервал 5 минут. 5 минут — это интервал хранения данных и отправки метрик. Если статистика агентов за оперативный интервал различается, то сервер выбирает статистику с лушчим показателем доступности.

### Статистика статусов

Агенты Апметра собирают статистику доступности за интервал накопления 30 секунд и отправляют ее на сервер. Чтобы собрать статистику, агент накапливает результаты проб в *массив статусов* для каждой пробы и группы целиком, и из этого массива собирает статистику.

Агент запускает проверки и записывает статусы проб в промежуточную таблицу. В этой таблице статус пробы обновляется с частотой запуска ее проверок. Интервал самой частой проверки — 200 мс. Поэтому каждые 200 мс агент собирает срез статусов с таблицы и добавляет их массивы статусов для каждой пробы. За 30 секунд будет 150 срезов статусов, это длина массива статусов у каждой пробы. По этим 150 статусам собирается статистика в единицах времени.

Пример массива статусов, где шаг в массиве 200 мс.

```
                       |------------ интервал накопления -----------|
запуск проверки        ↓________      ↓________      ↓________      |
статус проверки       o>________.______________X______________._____|
                                ↓              ↓              ↓
массив статусов       [ooooooooo...............XXXXXXXXXXXXXXX......]
N                          9          15              15         6

o — Nodata  = 9           ×200 мс = 1,8 c
. — Up      = 15 + 6 = 21 ×200 мс = 4,2 c
X — Down    = 15          ×200 мс = 3,0 c
u — Unknown = 0
```

Статус *Nodata* возникает, когда агент запускается не точно в начале интервала накопления, поэтому начальная часть интервала остается без данных.

Массив статусов группы вычисляется в агенте из массивов статусов проб. Возможно, что две пробы будут иметь статус *Down* в разное время. Тогда статистика доступности у группы будет хуже, чем у каждой из проб. Пример такой ситуации на массивах статусов:

```
             Массивы статусов   Доступность up/(up+down)
проба 1      ....XXXX.......    11/15 = 0.73
проба 2      ......XXXXX....    10/15 = 0.67
группа       ....XXXXXXX....     8/15 = 0.53
```

## Метрики

Сервер Апметра генерирует метрику *status_time* (тип *gauge*) — это время одного статуса за период 5 минут. Статусы в метриках есть для индивидуальных проб и для групп целиком. Сумма значений метрики по статусам для пробы или группы — всегда пять минут. Единицы значений — миллисекунды. Сервер отправляет метрики в формате протокола [remote_write](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write).

Пример метрик для пробы и группы целиком:

```promql
# Пример для пробы
status_time{group="synthetic", probe="dns", status="up"}        # ≤ 300000

# Пример для группы
status_time{group="synthetic", probe="__total__", status="up"}  # ≤ 300000

# Сумма статусов в точке всегда равна 5 минутам
sum by (status) (status_time{group="synthetic", probe="dns"})   # = 300000
```

## Вычисление доступности

Рассмотрим пример. Возьмем для простоты только статусы *up* и *down*. В пяти минутах 300'000 миллисекунд, в этих единицах приходит значение. Допустим, в точке метрики пришли значения с лейблами

    100000 status=down
    200000 status=up

тогда доступность группы или пробы за 5 минут будет равна

    availability = up/(down + up) = ⅔ = 66%

Чтобы показать доступность в веб-интерфейсах дашборда и статус-страницы, апметр оценивает время доступности с участием статуса *Unknown*:

    availability = (up + unknown)/(down + up + unknown)

*Unknown* учитывается, чтобы охватить больше времени измерения. Статус *Unknown* бывает по двум причинам: штатной недоступности компонента и из-за простоя зависимости в этой проверке.

Статус *Unknown* может свидетельствовать о штатной недоступном функциональности или компонента. Например, перезапуск Pod'а Декхауса во время обновления. Пока Декхаус инициализирует модули, Pod еще не в состоянии Ready. Обновление Декхаус может занять несколько минут. В этот период проверка Апметра не считает Декхаус недоступным, пока это время не превысило порог.

Статус *Unknown* получат те проверки, которые проверяют состояние кластерных ресурсов во время недоступности кластерного API-сервера. Это не говорит о том, доступна или нет сама проверяемая функциональность. Однако недоступность группы `control-plane` будет зафиксирована в любом случае. Когда не работает кластерный API-сервер, проба `control-plane/apiserver` будет в состоянии *Down*. Поэтому и вся группа `control-plane` будет иметь статус *Down*. В соглашении о доступности Deckhouse Enterprise Edition сумма компенсации за нарушение SLA считается по одной группе доступности, в которой эта сумма наибольшая. Поэтому мы не учитываем недоступность дважды, если даже проверяемая функциональность фактически была недоступной одновременно с недоступным API-сервером.

## Группы доступности и пробы

Бо̀льшая часть проверок доступности состоит из запросов к API-серверу кластера. В этих проверках предварительно проверяется доступность API-сервера запросом на `/version`. Проверка завершается в статусе *Unknown*, если API-сервер недоступен.

### Группа control-plane

#### apiserver

Проверяет доступность `kube-apiserver`. Делает запрос на `/version`.

Интервал: 5 секунд

#### basic-functionality

Проверяет работоспособность kube-apiserver на цикле жизни ConfigMap. Создает ConfigMap, удостоверятся в ее наличии, удаляет ConfigMap, проверяет, что ее нет.

Предварительно проверяет доступность API-сервера. Если API-сервер недоступен, статус пробы становится *Unknown*.

Интервал: 5 секунд

#### namespace

Проверяет работоспособность kube-apiserver на цикле жизни Namespace. Создает Namespace, удостоверятся в его наличии, удаляет Namespace, проверить что его нет.

Предварительно проверяет доступность API-сервер недоступен, статус пробы становится *Unknown*.

Интервал: 1 минута

#### controller-manager

Проверяет работоспособность kube-controller-manager на жизненном цикле пода контроллера. Создает StatefulSet, проверяет что создался Pod (его состояние нам не важно, может оставаться в статусе Pending). Удаляет StatefulSet, проверяет Pod удалился.

Предварительно проверяет доступность API-сервер недоступен, статус пробы становится *Unknown*.

Интервал: 1 минута

#### scheduler

Проверяет работоспособность kube-scheduler. Создает Pod и проверяет заполненность поля `.spec.nodeName`.

Предварительно проверяет доступность API-сервер недоступен, статус пробы становится *Unknown*.

Интервал: 1 минута

### Группа deckhouse

#### cluster-configuration

Проверяет работу конфигурации кластера. Декхаус управляет конфигурацией за счет хуков, поэтому эта проба проверяет работу хука.

Для этой пробы существует CRD [UpmeterHookProbe](cr.html#upmeterhookprobe). Модуль создает CR `UpmeterHookProbe`, если его нет, по одному на каждый Pod `upmeter-agent`.


В Custom Resource Апметр перезаписывает поле `spec.initial` случайным значением и ждет, пока поле `spec.mirror` станет равным этому же значению. Хук Декхауса дублирует значение из этого поля в поле `spec.mirror`. Если значение синхронизируется в течение 30 секунд, проба принимает статус *Up*.

Период: 1 минута. Предварительно проверяет доступность API-сервер недоступен, статус пробы становится *Unknown*.

Предварительно проверяет статус пода Декхауса:

* если статус пода Terminating, статус пробы становится *Unknown*,
* если статус пода Running, но не Ready менее 20 минут, статус пробы становится *Unknown*,
* если статус пода Running, но не Ready более 20 минут, статус пробы становится *Down*.

### Группа extensions

#### cluster-autoscaler

Проверяет, что хотя бы один Pod `cluster-autoscaler` находится в состоянии Ready.

Интервал: 10 секунд

#### cluster-scaling

Состоит из трех независимых проверок. Проверяет, что хотя бы один Pod в состоянии Ready у Deployment `machine-controller-manager`, `cloud-controller-manager`, `bashible-apiserver`.


Интервал: 10 секунд. Предварительно проверяет доступность API-сервера в каждой проверке.


#### grafana

Проверяет, что хотя бы один Pod Grafana в состоянии Ready.

Интервал: 10 секунд. Предварительно проверяет доступность API-сервера.

#### openvpn

Проверяет, что хотя бы один Pod OpenVPN в состоянии Ready.

Интервал: 10 секунд

#### prometheus-longterm

Состоит из двух проверок:

- хотя бы один Pod StatefulSet `prometheus-longterm` в состоянии Ready, предварительно проверяет доступность API-сервера;
- API отвечает "1" через сервис на запрос `/api/v1/query?query=vector(1)`.

Интервал обеих проверок: 10 секунд.

#### dashboard

Хотя бы один Pod Dashboard в состоянии Ready.

Интервал: 10 секунд. Предварительно проверяет доступность API-сервера.

#### dex

Состоит из двух проверок:

- хотя бы один Pod Dex в состоянии Ready, предварительно проверяет доступность API-сервера;
- API отвечает через сервис на запрос `/keys`.

Интервал обеих проверок: 10 секунд.

### Группа load-balancing

#### load-balancer-configuration

Хотя бы один Pod `cloud-controller-manager` в состоянии Ready

Интервал: 10 секунд. Предварительно проверяет доступность API-сервера.


#### metallb

Состоит из двух проверок:

- хотя бы один Pod MetalLB Controller в состоянии Ready
- хотя бы один Pod MetalLB Speaker в состоянии Ready

Интервал обеих проверок: 10 секунд. Предварительно проверяет доступность API-сервера в обеих проверках.

### Группа monitoring-and-autoscaling

#### prometheus

Состоит из двух проверок:

- хотя бы один Pod StatefulSet `prometheus-main` в состоянии Ready, предварительно проверяет доступность API-сервера;
- API отвечает "1" через сервис на запрос /api/v1/query?query=vector(1)

Интервал обеих проверок: 10 секунд.

#### trickster

Состоит из двух проверок:

- хотя бы один Pod Deployment `trickster` в состоянии Ready, предварительно проверяет доступность API-сервера;
- API отвечает "1" через сервис на запрос `/trickster/main/api/v1/query?query=vector(1)`

Интервал обеих проверок: 10 секунд.

#### prometheus-metrics-adapter

Состоит из двух проверок:

- хотя бы один Pod Deployment `prometheus-metrics-adapter` в состоянии Ready, предварительно проверяет доступность API-сервера;
- API отвечает "1" через сервис на запрос `/apis/custom.metrics.k8s.io/v1beta1/namespaces/d8-upmeter/metrics/memory_1m`

Интервал обеих проверок: 5 секунд.

#### vertical-pod-autoscaler

Состоит из трех независимых проверок.  Проверяет, что хотя бы один Pod в состоянии Ready у Deployment `vpa-updater`, `vpa-recommender`, `vpa-admission-controller`.

Интервал: 10 секунд. Предварительно проверяет доступность API-сервера во всех трех проверках.

#### metric-sources

- Хотя бы один под `kube-state-metrics` в состоянии Ready.
- Все Pod'ы DaemonSet `node-exporter` в состоянии Ready на всех узлах, где должны быть, однако не учитываются узлы:
  - которым меньше 10 минут
  - которые в процессе удаления (`deletionTimestamp`)
  - в состоянии `Unschedulable`

Интервал обеих проверок: 10 секунд. Предварительно проверяет доступность API-сервера во всех трех проверках.

#### key-metrics-present

- в prometheus есть метрики от `kube-state-metrics`
- в prometheus есть метрики от `node-exporter`
- в prometheus есть метрики от `kubelet`

Интервал всех проверок: 15 секунд.

#### Horizontal-pod-autoscaler (вычисляемая проба)

Это вычисляемая проба. Она не делает самостоятельных проверок, только комбинирует статусы других: собирает свой массив статусов и массивов других проб.

- `control-plane/controller-manager`
- `monitoring-and-autoscaling/prometheus-metrics-adapter`

### Группа Nginx

Пробы создаются динамически для имеющихся контроллеров. Каждая проба называется по имени контроллера. Она проверяет, что хотя бы один Pod в состоянии Ready. Предварительно проверяет доступность API-сервера.

Интервал: 5 секунд.

### Группа Node Groups

Пробы создаются динамически для NodeGroup, у которых `NG.spec.CloudInstances.minPerZone` > 0. Каждая проба называется по имени NodeGroup. Она проверяет, что количество узлов в NodeGroup соответствует ожидаемому в каждой зоне. Предварительно проверяет доступность API-сервера.

Интервал: 10 секунд.

### Группа Synthetic

Группа нацелена отслеживать сетевую связность между узлами, покрывая все пары узлов со временем в случайном порядке. Проверка проходит в сети Pod'ов.

Для проверок запускается модульное приложение `smoke-mini`, пять StatefulSet'ов по одному реплике. Пробы проверяют связность как между пятью Pod'ами `smoke-mini`, так и с Pod'ами `upmeter-agent`, работающих на master-узлах. Раз в минуту один из Pod'ов `smoke-mini` переносится на другой узел. Максимально могут быть задействованы 8 узлов: 3 master-узла и 5 не master-узлов.

`smoke-mini` не масштабируется с размером кластера. Поэтому для больших кластеров он не приносит оперативную информацию о связности сети. Недоступность сети подов у малого количества узлов в большом кластере имеет слабый эффект на статусы проб.

#### access

Проверяет, что хотя бы один Pod `smoke-mini` отвечает HTTP-статусом 200. Список Pod'ов берется из DNS.

Интервал: 5 секунд.

#### dns

Две проверки:

- хотя бы один Pod `smoke-mini` отвечает HTTP-статусом 200 на `/dns`. Он резолвит `kubernetes.default`.
- `upmeter-agent` резолвит внутренний домен `kubernetes.default.svc.<<global.discovery.clusterDomain>>`

Интервал: 200 миллисекунд.

#### neighbour

Хотя бы один Pod `smoke-mini` отвечает HTTP-статусом 200 на `/neighbor`. Список Pod’ов берется из DNS. `smoke-mini` опрашивает соседние Pod'ы по имени headless-сервиса.

Интервал: 5 секунд.

#### neighbour-via-service

Хотя бы один Pod `smoke-mini` отвечает HTTP-статусом 200 на `/neighbour-via-service`. Список Pod’ов берется из DNS.
`smoke-mini` делает 4 запроса на общий сервис `ClusterIP` и отвечает 200, если было не больше 2 ошибок.

Интервал: 5 секунд.
